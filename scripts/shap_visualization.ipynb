# shap_visualization.ipynb

# ============================
# SHAP Visualization Notebook
# For AI-DAC / TLL Framework
# ============================

# Author: William K.
# Purpose: Visualize feature importance and interpret anomaly predictions
# Dataset: sql_train.csv (structured logs with anomaly labels)

# --- STEP 1: IMPORTS ---
import shap
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import joblib

from sklearn.preprocessing import StandardScaler
from keras.models import load_model

# --- STEP 2: LOAD DATA ---
df = pd.read_csv('datasets/sql_train.csv')
print("Dataset shape:", df.shape)
df.head()

# --- STEP 3: PREPROCESS ---
# Assume label column is 'anomaly' and features are all others
features = df.drop(columns=['anomaly'])
labels = df['anomaly']

# Load the scaler (from training)
scaler = joblib.load('models/supervised/scaler.pkl')
X_scaled = scaler.transform(features)

# --- STEP 4: LOAD TRAINED MODEL ---
# Example: Pretrained LSTM or dense NN (e.g., lstm_v4.h5)
model = load_model('models/supervised/lstm_v4.h5')
print(model.summary())

# --- STEP 5: CREATE EXPLAINER ---
# SHAP requires a model wrapper for deep learning models
# We use the DeepExplainer for Keras models
explainer = shap.DeepExplainer(model, X_scaled[:100])  # use a background sample
shap_values = explainer.shap_values(X_scaled[:200])    # limit to 200 examples for performance

# --- STEP 6: SHAP SUMMARY PLOT ---
# Displays mean absolute SHAP values per feature
shap.summary_plot(shap_values[0], features.iloc[:200], show=True)

# --- STEP 7: DEPENDENCE PLOT (OPTIONAL) ---
# Show effect of individual features on prediction
shap.dependence_plot('duration', shap_values[0], features.iloc[:200])

# --- STEP 8: FORCE PLOT (INTERPRET INDIVIDUAL PREDICTION) ---
# For Jupyter interactivity
shap.initjs()
i = 10  # example index
shap.force_plot(
    explainer.expected_value[0],
    shap_values[0][i],
    features.iloc[i],
    matplotlib=True
)
